{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import jovian\n",
    "import torchvision\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import make_grid\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"diabetes.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(\"Outcome\",axis=1).values\n",
    "y=df[\"Outcome\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.FloatTensor(x_train)\n",
    "Y_train=torch.LongTensor(y_train)\n",
    "X_val=torch.FloatTensor(x_val)\n",
    "Y_val=torch.LongTensor(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=8\n",
    "output_size=2\n",
    "hidden_size1=20\n",
    "hidden_size2=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diab(nn.Module):\n",
    "    def __init__(self,input_size=8,hidden_size1=100,hidden_size2=100,output_size=2) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(input_size,hidden_size1)\n",
    "        self.linear2=nn.Linear(hidden_size1,hidden_size2)\n",
    "        self.linear3=nn.Linear(hidden_size2,output_size)\n",
    "    def forward(self,xb):\n",
    "        out=self.linear1(xb)\n",
    "        out=F.relu(out)\n",
    "        out=self.linear2(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.linear3(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "model=diab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of diab(\n",
       "  (linear1): Linear(in_features=8, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (linear3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500\n",
    "history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 8.9627\n",
      "Epoch [2], val_loss: 25.5037\n",
      "Epoch [3], val_loss: 17.1556\n",
      "Epoch [4], val_loss: 9.1673\n",
      "Epoch [5], val_loss: 3.0857\n",
      "Epoch [6], val_loss: 2.5975\n",
      "Epoch [7], val_loss: 4.1389\n",
      "Epoch [8], val_loss: 4.1524\n",
      "Epoch [9], val_loss: 3.2180\n",
      "Epoch [10], val_loss: 2.0692\n",
      "Epoch [11], val_loss: 1.1916\n",
      "Epoch [12], val_loss: 0.7924\n",
      "Epoch [13], val_loss: 0.7367\n",
      "Epoch [14], val_loss: 0.8178\n",
      "Epoch [15], val_loss: 0.8702\n",
      "Epoch [16], val_loss: 0.8616\n",
      "Epoch [17], val_loss: 0.8266\n",
      "Epoch [18], val_loss: 0.7856\n",
      "Epoch [19], val_loss: 0.7475\n",
      "Epoch [20], val_loss: 0.7159\n",
      "Epoch [21], val_loss: 0.6924\n",
      "Epoch [22], val_loss: 0.6763\n",
      "Epoch [23], val_loss: 0.6664\n",
      "Epoch [24], val_loss: 0.6612\n",
      "Epoch [25], val_loss: 0.6589\n",
      "Epoch [26], val_loss: 0.6584\n",
      "Epoch [27], val_loss: 0.6588\n",
      "Epoch [28], val_loss: 0.6597\n",
      "Epoch [29], val_loss: 0.6606\n",
      "Epoch [30], val_loss: 0.6614\n",
      "Epoch [31], val_loss: 0.6619\n",
      "Epoch [32], val_loss: 0.6620\n",
      "Epoch [33], val_loss: 0.6617\n",
      "Epoch [34], val_loss: 0.6609\n",
      "Epoch [35], val_loss: 0.6593\n",
      "Epoch [36], val_loss: 0.6572\n",
      "Epoch [37], val_loss: 0.6547\n",
      "Epoch [38], val_loss: 0.6520\n",
      "Epoch [39], val_loss: 0.6491\n",
      "Epoch [40], val_loss: 0.6469\n",
      "Epoch [41], val_loss: 0.6458\n",
      "Epoch [42], val_loss: 0.6454\n",
      "Epoch [43], val_loss: 0.6448\n",
      "Epoch [44], val_loss: 0.6432\n",
      "Epoch [45], val_loss: 0.6409\n",
      "Epoch [46], val_loss: 0.6388\n",
      "Epoch [47], val_loss: 0.6372\n",
      "Epoch [48], val_loss: 0.6363\n",
      "Epoch [49], val_loss: 0.6359\n",
      "Epoch [50], val_loss: 0.6354\n",
      "Epoch [51], val_loss: 0.6345\n",
      "Epoch [52], val_loss: 0.6332\n",
      "Epoch [53], val_loss: 0.6314\n",
      "Epoch [54], val_loss: 0.6296\n",
      "Epoch [55], val_loss: 0.6279\n",
      "Epoch [56], val_loss: 0.6266\n",
      "Epoch [57], val_loss: 0.6256\n",
      "Epoch [58], val_loss: 0.6245\n",
      "Epoch [59], val_loss: 0.6233\n",
      "Epoch [60], val_loss: 0.6218\n",
      "Epoch [61], val_loss: 0.6204\n",
      "Epoch [62], val_loss: 0.6193\n",
      "Epoch [63], val_loss: 0.6184\n",
      "Epoch [64], val_loss: 0.6174\n",
      "Epoch [65], val_loss: 0.6164\n",
      "Epoch [66], val_loss: 0.6152\n",
      "Epoch [67], val_loss: 0.6141\n",
      "Epoch [68], val_loss: 0.6130\n",
      "Epoch [69], val_loss: 0.6120\n",
      "Epoch [70], val_loss: 0.6110\n",
      "Epoch [71], val_loss: 0.6101\n",
      "Epoch [72], val_loss: 0.6091\n",
      "Epoch [73], val_loss: 0.6080\n",
      "Epoch [74], val_loss: 0.6071\n",
      "Epoch [75], val_loss: 0.6063\n",
      "Epoch [76], val_loss: 0.6054\n",
      "Epoch [77], val_loss: 0.6045\n",
      "Epoch [78], val_loss: 0.6036\n",
      "Epoch [79], val_loss: 0.6027\n",
      "Epoch [80], val_loss: 0.6020\n",
      "Epoch [81], val_loss: 0.6011\n",
      "Epoch [82], val_loss: 0.6002\n",
      "Epoch [83], val_loss: 0.5994\n",
      "Epoch [84], val_loss: 0.5987\n",
      "Epoch [85], val_loss: 0.5979\n",
      "Epoch [86], val_loss: 0.5971\n",
      "Epoch [87], val_loss: 0.5963\n",
      "Epoch [88], val_loss: 0.5955\n",
      "Epoch [89], val_loss: 0.5947\n",
      "Epoch [90], val_loss: 0.5939\n",
      "Epoch [91], val_loss: 0.5931\n",
      "Epoch [92], val_loss: 0.5923\n",
      "Epoch [93], val_loss: 0.5916\n",
      "Epoch [94], val_loss: 0.5909\n",
      "Epoch [95], val_loss: 0.5901\n",
      "Epoch [96], val_loss: 0.5894\n",
      "Epoch [97], val_loss: 0.5887\n",
      "Epoch [98], val_loss: 0.5881\n",
      "Epoch [99], val_loss: 0.5875\n",
      "Epoch [100], val_loss: 0.5869\n",
      "Epoch [101], val_loss: 0.5863\n",
      "Epoch [102], val_loss: 0.5856\n",
      "Epoch [103], val_loss: 0.5850\n",
      "Epoch [104], val_loss: 0.5844\n",
      "Epoch [105], val_loss: 0.5838\n",
      "Epoch [106], val_loss: 0.5832\n",
      "Epoch [107], val_loss: 0.5826\n",
      "Epoch [108], val_loss: 0.5820\n",
      "Epoch [109], val_loss: 0.5815\n",
      "Epoch [110], val_loss: 0.5809\n",
      "Epoch [111], val_loss: 0.5804\n",
      "Epoch [112], val_loss: 0.5799\n",
      "Epoch [113], val_loss: 0.5794\n",
      "Epoch [114], val_loss: 0.5789\n",
      "Epoch [115], val_loss: 0.5784\n",
      "Epoch [116], val_loss: 0.5779\n",
      "Epoch [117], val_loss: 0.5774\n",
      "Epoch [118], val_loss: 0.5769\n",
      "Epoch [119], val_loss: 0.5764\n",
      "Epoch [120], val_loss: 0.5759\n",
      "Epoch [121], val_loss: 0.5754\n",
      "Epoch [122], val_loss: 0.5749\n",
      "Epoch [123], val_loss: 0.5745\n",
      "Epoch [124], val_loss: 0.5740\n",
      "Epoch [125], val_loss: 0.5735\n",
      "Epoch [126], val_loss: 0.5730\n",
      "Epoch [127], val_loss: 0.5726\n",
      "Epoch [128], val_loss: 0.5721\n",
      "Epoch [129], val_loss: 0.5717\n",
      "Epoch [130], val_loss: 0.5712\n",
      "Epoch [131], val_loss: 0.5707\n",
      "Epoch [132], val_loss: 0.5702\n",
      "Epoch [133], val_loss: 0.5698\n",
      "Epoch [134], val_loss: 0.5693\n",
      "Epoch [135], val_loss: 0.5689\n",
      "Epoch [136], val_loss: 0.5684\n",
      "Epoch [137], val_loss: 0.5679\n",
      "Epoch [138], val_loss: 0.5674\n",
      "Epoch [139], val_loss: 0.5669\n",
      "Epoch [140], val_loss: 0.5664\n",
      "Epoch [141], val_loss: 0.5659\n",
      "Epoch [142], val_loss: 0.5654\n",
      "Epoch [143], val_loss: 0.5649\n",
      "Epoch [144], val_loss: 0.5644\n",
      "Epoch [145], val_loss: 0.5639\n",
      "Epoch [146], val_loss: 0.5634\n",
      "Epoch [147], val_loss: 0.5629\n",
      "Epoch [148], val_loss: 0.5624\n",
      "Epoch [149], val_loss: 0.5619\n",
      "Epoch [150], val_loss: 0.5614\n",
      "Epoch [151], val_loss: 0.5609\n",
      "Epoch [152], val_loss: 0.5604\n",
      "Epoch [153], val_loss: 0.5599\n",
      "Epoch [154], val_loss: 0.5594\n",
      "Epoch [155], val_loss: 0.5589\n",
      "Epoch [156], val_loss: 0.5583\n",
      "Epoch [157], val_loss: 0.5578\n",
      "Epoch [158], val_loss: 0.5572\n",
      "Epoch [159], val_loss: 0.5567\n",
      "Epoch [160], val_loss: 0.5561\n",
      "Epoch [161], val_loss: 0.5555\n",
      "Epoch [162], val_loss: 0.5550\n",
      "Epoch [163], val_loss: 0.5544\n",
      "Epoch [164], val_loss: 0.5538\n",
      "Epoch [165], val_loss: 0.5532\n",
      "Epoch [166], val_loss: 0.5526\n",
      "Epoch [167], val_loss: 0.5520\n",
      "Epoch [168], val_loss: 0.5514\n",
      "Epoch [169], val_loss: 0.5508\n",
      "Epoch [170], val_loss: 0.5501\n",
      "Epoch [171], val_loss: 0.5494\n",
      "Epoch [172], val_loss: 0.5486\n",
      "Epoch [173], val_loss: 0.5479\n",
      "Epoch [174], val_loss: 0.5471\n",
      "Epoch [175], val_loss: 0.5465\n",
      "Epoch [176], val_loss: 0.5458\n",
      "Epoch [177], val_loss: 0.5450\n",
      "Epoch [178], val_loss: 0.5443\n",
      "Epoch [179], val_loss: 0.5434\n",
      "Epoch [180], val_loss: 0.5427\n",
      "Epoch [181], val_loss: 0.5419\n",
      "Epoch [182], val_loss: 0.5411\n",
      "Epoch [183], val_loss: 0.5404\n",
      "Epoch [184], val_loss: 0.5397\n",
      "Epoch [185], val_loss: 0.5391\n",
      "Epoch [186], val_loss: 0.5386\n",
      "Epoch [187], val_loss: 0.5376\n",
      "Epoch [188], val_loss: 0.5371\n",
      "Epoch [189], val_loss: 0.5369\n",
      "Epoch [190], val_loss: 0.5356\n",
      "Epoch [191], val_loss: 0.5352\n",
      "Epoch [192], val_loss: 0.5349\n",
      "Epoch [193], val_loss: 0.5336\n",
      "Epoch [194], val_loss: 0.5344\n",
      "Epoch [195], val_loss: 0.5322\n",
      "Epoch [196], val_loss: 0.5324\n",
      "Epoch [197], val_loss: 0.5308\n",
      "Epoch [198], val_loss: 0.5308\n",
      "Epoch [199], val_loss: 0.5301\n",
      "Epoch [200], val_loss: 0.5293\n",
      "Epoch [201], val_loss: 0.5291\n",
      "Epoch [202], val_loss: 0.5281\n",
      "Epoch [203], val_loss: 0.5279\n",
      "Epoch [204], val_loss: 0.5271\n",
      "Epoch [205], val_loss: 0.5268\n",
      "Epoch [206], val_loss: 0.5261\n",
      "Epoch [207], val_loss: 0.5257\n",
      "Epoch [208], val_loss: 0.5250\n",
      "Epoch [209], val_loss: 0.5246\n",
      "Epoch [210], val_loss: 0.5241\n",
      "Epoch [211], val_loss: 0.5235\n",
      "Epoch [212], val_loss: 0.5231\n",
      "Epoch [213], val_loss: 0.5225\n",
      "Epoch [214], val_loss: 0.5221\n",
      "Epoch [215], val_loss: 0.5214\n",
      "Epoch [216], val_loss: 0.5210\n",
      "Epoch [217], val_loss: 0.5204\n",
      "Epoch [218], val_loss: 0.5200\n",
      "Epoch [219], val_loss: 0.5194\n",
      "Epoch [220], val_loss: 0.5191\n",
      "Epoch [221], val_loss: 0.5184\n",
      "Epoch [222], val_loss: 0.5179\n",
      "Epoch [223], val_loss: 0.5173\n",
      "Epoch [224], val_loss: 0.5168\n",
      "Epoch [225], val_loss: 0.5162\n",
      "Epoch [226], val_loss: 0.5157\n",
      "Epoch [227], val_loss: 0.5153\n",
      "Epoch [228], val_loss: 0.5148\n",
      "Epoch [229], val_loss: 0.5143\n",
      "Epoch [230], val_loss: 0.5139\n",
      "Epoch [231], val_loss: 0.5134\n",
      "Epoch [232], val_loss: 0.5129\n",
      "Epoch [233], val_loss: 0.5125\n",
      "Epoch [234], val_loss: 0.5120\n",
      "Epoch [235], val_loss: 0.5116\n",
      "Epoch [236], val_loss: 0.5111\n",
      "Epoch [237], val_loss: 0.5107\n",
      "Epoch [238], val_loss: 0.5102\n",
      "Epoch [239], val_loss: 0.5098\n",
      "Epoch [240], val_loss: 0.5093\n",
      "Epoch [241], val_loss: 0.5089\n",
      "Epoch [242], val_loss: 0.5084\n",
      "Epoch [243], val_loss: 0.5080\n",
      "Epoch [244], val_loss: 0.5076\n",
      "Epoch [245], val_loss: 0.5072\n",
      "Epoch [246], val_loss: 0.5067\n",
      "Epoch [247], val_loss: 0.5063\n",
      "Epoch [248], val_loss: 0.5059\n",
      "Epoch [249], val_loss: 0.5055\n",
      "Epoch [250], val_loss: 0.5051\n",
      "Epoch [251], val_loss: 0.5046\n",
      "Epoch [252], val_loss: 0.5042\n",
      "Epoch [253], val_loss: 0.5038\n",
      "Epoch [254], val_loss: 0.5034\n",
      "Epoch [255], val_loss: 0.5030\n",
      "Epoch [256], val_loss: 0.5026\n",
      "Epoch [257], val_loss: 0.5022\n",
      "Epoch [258], val_loss: 0.5017\n",
      "Epoch [259], val_loss: 0.5013\n",
      "Epoch [260], val_loss: 0.5009\n",
      "Epoch [261], val_loss: 0.5006\n",
      "Epoch [262], val_loss: 0.5004\n",
      "Epoch [263], val_loss: 0.4998\n",
      "Epoch [264], val_loss: 0.4993\n",
      "Epoch [265], val_loss: 0.4989\n",
      "Epoch [266], val_loss: 0.4985\n",
      "Epoch [267], val_loss: 0.4983\n",
      "Epoch [268], val_loss: 0.4978\n",
      "Epoch [269], val_loss: 0.4974\n",
      "Epoch [270], val_loss: 0.4969\n",
      "Epoch [271], val_loss: 0.4964\n",
      "Epoch [272], val_loss: 0.4960\n",
      "Epoch [273], val_loss: 0.4956\n",
      "Epoch [274], val_loss: 0.4953\n",
      "Epoch [275], val_loss: 0.4950\n",
      "Epoch [276], val_loss: 0.4946\n",
      "Epoch [277], val_loss: 0.4943\n",
      "Epoch [278], val_loss: 0.4940\n",
      "Epoch [279], val_loss: 0.4934\n",
      "Epoch [280], val_loss: 0.4928\n",
      "Epoch [281], val_loss: 0.4923\n",
      "Epoch [282], val_loss: 0.4919\n",
      "Epoch [283], val_loss: 0.4916\n",
      "Epoch [284], val_loss: 0.4915\n",
      "Epoch [285], val_loss: 0.4910\n",
      "Epoch [286], val_loss: 0.4904\n",
      "Epoch [287], val_loss: 0.4899\n",
      "Epoch [288], val_loss: 0.4894\n",
      "Epoch [289], val_loss: 0.4890\n",
      "Epoch [290], val_loss: 0.4887\n",
      "Epoch [291], val_loss: 0.4883\n",
      "Epoch [292], val_loss: 0.4878\n",
      "Epoch [293], val_loss: 0.4874\n",
      "Epoch [294], val_loss: 0.4872\n",
      "Epoch [295], val_loss: 0.4874\n",
      "Epoch [296], val_loss: 0.4869\n",
      "Epoch [297], val_loss: 0.4861\n",
      "Epoch [298], val_loss: 0.4854\n",
      "Epoch [299], val_loss: 0.4854\n",
      "Epoch [300], val_loss: 0.4855\n",
      "Epoch [301], val_loss: 0.4847\n",
      "Epoch [302], val_loss: 0.4838\n",
      "Epoch [303], val_loss: 0.4836\n",
      "Epoch [304], val_loss: 0.4834\n",
      "Epoch [305], val_loss: 0.4828\n",
      "Epoch [306], val_loss: 0.4823\n",
      "Epoch [307], val_loss: 0.4820\n",
      "Epoch [308], val_loss: 0.4816\n",
      "Epoch [309], val_loss: 0.4810\n",
      "Epoch [310], val_loss: 0.4806\n",
      "Epoch [311], val_loss: 0.4803\n",
      "Epoch [312], val_loss: 0.4798\n",
      "Epoch [313], val_loss: 0.4794\n",
      "Epoch [314], val_loss: 0.4790\n",
      "Epoch [315], val_loss: 0.4786\n",
      "Epoch [316], val_loss: 0.4782\n",
      "Epoch [317], val_loss: 0.4778\n",
      "Epoch [318], val_loss: 0.4776\n",
      "Epoch [319], val_loss: 0.4774\n",
      "Epoch [320], val_loss: 0.4777\n",
      "Epoch [321], val_loss: 0.4777\n",
      "Epoch [322], val_loss: 0.4768\n",
      "Epoch [323], val_loss: 0.4755\n",
      "Epoch [324], val_loss: 0.4752\n",
      "Epoch [325], val_loss: 0.4759\n",
      "Epoch [326], val_loss: 0.4764\n",
      "Epoch [327], val_loss: 0.4759\n",
      "Epoch [328], val_loss: 0.4743\n",
      "Epoch [329], val_loss: 0.4731\n",
      "Epoch [330], val_loss: 0.4736\n",
      "Epoch [331], val_loss: 0.4740\n",
      "Epoch [332], val_loss: 0.4725\n",
      "Epoch [333], val_loss: 0.4713\n",
      "Epoch [334], val_loss: 0.4716\n",
      "Epoch [335], val_loss: 0.4723\n",
      "Epoch [336], val_loss: 0.4721\n",
      "Epoch [337], val_loss: 0.4703\n",
      "Epoch [338], val_loss: 0.4693\n",
      "Epoch [339], val_loss: 0.4700\n",
      "Epoch [340], val_loss: 0.4704\n",
      "Epoch [341], val_loss: 0.4700\n",
      "Epoch [342], val_loss: 0.4680\n",
      "Epoch [343], val_loss: 0.4670\n",
      "Epoch [344], val_loss: 0.4669\n",
      "Epoch [345], val_loss: 0.4671\n",
      "Epoch [346], val_loss: 0.4673\n",
      "Epoch [347], val_loss: 0.4669\n",
      "Epoch [348], val_loss: 0.4662\n",
      "Epoch [349], val_loss: 0.4650\n",
      "Epoch [350], val_loss: 0.4642\n",
      "Epoch [351], val_loss: 0.4635\n",
      "Epoch [352], val_loss: 0.4630\n",
      "Epoch [353], val_loss: 0.4627\n",
      "Epoch [354], val_loss: 0.4626\n",
      "Epoch [355], val_loss: 0.4629\n",
      "Epoch [356], val_loss: 0.4637\n",
      "Epoch [357], val_loss: 0.4657\n",
      "Epoch [358], val_loss: 0.4660\n",
      "Epoch [359], val_loss: 0.4658\n",
      "Epoch [360], val_loss: 0.4638\n",
      "Epoch [361], val_loss: 0.4615\n",
      "Epoch [362], val_loss: 0.4595\n",
      "Epoch [363], val_loss: 0.4586\n",
      "Epoch [364], val_loss: 0.4588\n",
      "Epoch [365], val_loss: 0.4596\n",
      "Epoch [366], val_loss: 0.4609\n",
      "Epoch [367], val_loss: 0.4609\n",
      "Epoch [368], val_loss: 0.4611\n",
      "Epoch [369], val_loss: 0.4593\n",
      "Epoch [370], val_loss: 0.4578\n",
      "Epoch [371], val_loss: 0.4561\n",
      "Epoch [372], val_loss: 0.4548\n",
      "Epoch [373], val_loss: 0.4546\n",
      "Epoch [374], val_loss: 0.4553\n",
      "Epoch [375], val_loss: 0.4561\n",
      "Epoch [376], val_loss: 0.4569\n",
      "Epoch [377], val_loss: 0.4579\n",
      "Epoch [378], val_loss: 0.4574\n",
      "Epoch [379], val_loss: 0.4561\n",
      "Epoch [380], val_loss: 0.4531\n",
      "Epoch [381], val_loss: 0.4515\n",
      "Epoch [382], val_loss: 0.4508\n",
      "Epoch [383], val_loss: 0.4510\n",
      "Epoch [384], val_loss: 0.4524\n",
      "Epoch [385], val_loss: 0.4545\n",
      "Epoch [386], val_loss: 0.4567\n",
      "Epoch [387], val_loss: 0.4557\n",
      "Epoch [388], val_loss: 0.4531\n",
      "Epoch [389], val_loss: 0.4496\n",
      "Epoch [390], val_loss: 0.4480\n",
      "Epoch [391], val_loss: 0.4476\n",
      "Epoch [392], val_loss: 0.4481\n",
      "Epoch [393], val_loss: 0.4496\n",
      "Epoch [394], val_loss: 0.4497\n",
      "Epoch [395], val_loss: 0.4498\n",
      "Epoch [396], val_loss: 0.4489\n",
      "Epoch [397], val_loss: 0.4476\n",
      "Epoch [398], val_loss: 0.4459\n",
      "Epoch [399], val_loss: 0.4450\n",
      "Epoch [400], val_loss: 0.4447\n",
      "Epoch [401], val_loss: 0.4441\n",
      "Epoch [402], val_loss: 0.4435\n",
      "Epoch [403], val_loss: 0.4435\n",
      "Epoch [404], val_loss: 0.4437\n",
      "Epoch [405], val_loss: 0.4443\n",
      "Epoch [406], val_loss: 0.4467\n",
      "Epoch [407], val_loss: 0.4510\n",
      "Epoch [408], val_loss: 0.4570\n",
      "Epoch [409], val_loss: 0.4612\n",
      "Epoch [410], val_loss: 0.4561\n",
      "Epoch [411], val_loss: 0.4450\n",
      "Epoch [412], val_loss: 0.4403\n",
      "Epoch [413], val_loss: 0.4435\n",
      "Epoch [414], val_loss: 0.4478\n",
      "Epoch [415], val_loss: 0.4481\n",
      "Epoch [416], val_loss: 0.4440\n",
      "Epoch [417], val_loss: 0.4386\n",
      "Epoch [418], val_loss: 0.4390\n",
      "Epoch [419], val_loss: 0.4437\n",
      "Epoch [420], val_loss: 0.4456\n",
      "Epoch [421], val_loss: 0.4457\n",
      "Epoch [422], val_loss: 0.4397\n",
      "Epoch [423], val_loss: 0.4359\n",
      "Epoch [424], val_loss: 0.4365\n",
      "Epoch [425], val_loss: 0.4385\n",
      "Epoch [426], val_loss: 0.4408\n",
      "Epoch [427], val_loss: 0.4391\n",
      "Epoch [428], val_loss: 0.4351\n",
      "Epoch [429], val_loss: 0.4336\n",
      "Epoch [430], val_loss: 0.4337\n",
      "Epoch [431], val_loss: 0.4354\n",
      "Epoch [432], val_loss: 0.4363\n",
      "Epoch [433], val_loss: 0.4362\n",
      "Epoch [434], val_loss: 0.4347\n",
      "Epoch [435], val_loss: 0.4332\n",
      "Epoch [436], val_loss: 0.4308\n",
      "Epoch [437], val_loss: 0.4301\n",
      "Epoch [438], val_loss: 0.4307\n",
      "Epoch [439], val_loss: 0.4317\n",
      "Epoch [440], val_loss: 0.4351\n",
      "Epoch [441], val_loss: 0.4408\n",
      "Epoch [442], val_loss: 0.4429\n",
      "Epoch [443], val_loss: 0.4405\n",
      "Epoch [444], val_loss: 0.4335\n",
      "Epoch [445], val_loss: 0.4282\n",
      "Epoch [446], val_loss: 0.4264\n",
      "Epoch [447], val_loss: 0.4281\n",
      "Epoch [448], val_loss: 0.4315\n",
      "Epoch [449], val_loss: 0.4358\n",
      "Epoch [450], val_loss: 0.4409\n",
      "Epoch [451], val_loss: 0.4386\n",
      "Epoch [452], val_loss: 0.4337\n",
      "Epoch [453], val_loss: 0.4267\n",
      "Epoch [454], val_loss: 0.4233\n",
      "Epoch [455], val_loss: 0.4238\n",
      "Epoch [456], val_loss: 0.4276\n",
      "Epoch [457], val_loss: 0.4327\n",
      "Epoch [458], val_loss: 0.4353\n",
      "Epoch [459], val_loss: 0.4366\n",
      "Epoch [460], val_loss: 0.4252\n",
      "Epoch [461], val_loss: 0.4213\n",
      "Epoch [462], val_loss: 0.4239\n",
      "Epoch [463], val_loss: 0.4346\n",
      "Epoch [464], val_loss: 0.4487\n",
      "Epoch [465], val_loss: 0.4427\n",
      "Epoch [466], val_loss: 0.4273\n",
      "Epoch [467], val_loss: 0.4193\n",
      "Epoch [468], val_loss: 0.4220\n",
      "Epoch [469], val_loss: 0.4220\n",
      "Epoch [470], val_loss: 0.4248\n",
      "Epoch [471], val_loss: 0.4299\n",
      "Epoch [472], val_loss: 0.4209\n",
      "Epoch [473], val_loss: 0.4178\n",
      "Epoch [474], val_loss: 0.4174\n",
      "Epoch [475], val_loss: 0.4161\n",
      "Epoch [476], val_loss: 0.4180\n",
      "Epoch [477], val_loss: 0.4216\n",
      "Epoch [478], val_loss: 0.4208\n",
      "Epoch [479], val_loss: 0.4208\n",
      "Epoch [480], val_loss: 0.4212\n",
      "Epoch [481], val_loss: 0.4127\n",
      "Epoch [482], val_loss: 0.4122\n",
      "Epoch [483], val_loss: 0.4149\n",
      "Epoch [484], val_loss: 0.4185\n",
      "Epoch [485], val_loss: 0.4284\n",
      "Epoch [486], val_loss: 0.4371\n",
      "Epoch [487], val_loss: 0.4402\n",
      "Epoch [488], val_loss: 0.4248\n",
      "Epoch [489], val_loss: 0.4138\n",
      "Epoch [490], val_loss: 0.4083\n",
      "Epoch [491], val_loss: 0.4124\n",
      "Epoch [492], val_loss: 0.4211\n",
      "Epoch [493], val_loss: 0.4162\n",
      "Epoch [494], val_loss: 0.4112\n",
      "Epoch [495], val_loss: 0.4072\n",
      "Epoch [496], val_loss: 0.4045\n",
      "Epoch [497], val_loss: 0.4031\n",
      "Epoch [498], val_loss: 0.4025\n",
      "Epoch [499], val_loss: 0.4018\n",
      "Epoch [500], val_loss: 0.4014\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(epochs):\n",
    "    i=i+1\n",
    "    y_pred=model.forward(X_train)\n",
    "    loss=loss_function(y_pred,Y_train)\n",
    "    history.append(loss)\n",
    "    print(\"Epoch [{}], val_loss: {:.4f}\".format(i,loss.item()))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8290)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model.forward(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(X_val):\n",
    "        prediction=model.forward(data)\n",
    "        predictions.append(prediction.argmax().item())\n",
    "        print(prediction.argmax().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf=pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
